{"cells":[{"cell_type":"markdown","source":["# **Computational Creativity**\n","\n","---\n","\n","### **Generative Adversarial Recurrent Neural Networks for Text Generation**\n"],"metadata":{"id":"kYC1AVB8h5YN"}},{"cell_type":"markdown","source":["When testing trained models, scroll to the end to see results."],"metadata":{"id":"GRGHeZKaeCeu"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":323,"status":"ok","timestamp":1650470102039,"user":{"displayName":"Yang Li","userId":"01260798103734205900"},"user_tz":-60},"id":"hj8P9cGv8FBc","outputId":"9fae8e14-95bb-4efc-8abc-9a624aef9873"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found GPU at: /device:GPU:0.\n","TensorFlow version: 2.8.0\n"]}],"source":["# Imports and GPU setup\n","\n","import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from tqdm.autonotebook import tqdm\n","from google.colab import drive\n","\n","print(f\"Found GPU at: {tf.test.gpu_device_name()}.\\nTensorFlow version: {tf.__version__}\")"]},{"cell_type":"markdown","metadata":{"id":"XEkSB2qpx_ha"},"source":["# Dataset"]},{"cell_type":"markdown","metadata":{"id":"Lr5n6iS60tRl"},"source":["## <h1>Load text</h1> \n","\n","Load Choose a book from [Project Gutenberg](https://www.gutenberg.org/). For any book on the website, you can get to a link pointing to a plain-text version on its page. Copy the link into `book_choice`.\n","\n","The text will be converted into lower case."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":39,"status":"ok","timestamp":1650470102490,"user":{"displayName":"Yang Li","userId":"01260798103734205900"},"user_tz":-60},"id":"8V_z6ENgjWu_","outputId":"4d429a63-5690-4f68-d2f6-d58b3959fccb"},"outputs":[{"output_type":"stream","name":"stdout","text":["The dataset has 789417 characters in total.\n"]}],"source":["book_choice = \"https://www.gutenberg.org/files/1342/1342-0.txt\"  # @param {type: \"string\"}\n","path_to_file = tf.keras.utils.get_file(\"Book\", book_choice)\n","text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n","\n","# convert the text into lower case\n","text = text.lower()\n","\n","'''\n","# save the file\n","f = open(path + \"datasets/text.txt\", 'w')\n","f.write(text)\n","f.close()\n","'''\n","\n","# The length of text is the number of characters in it\n","print (\"The dataset has\", len(text), \"characters in total.\")"]},{"cell_type":"code","source":["'''\n","f = open(path + \"datasets/text.txt\", 'r')\n","text = f.read()\n","f.close()\n","'''"],"metadata":{"id":"PXNSyq5jggpT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wgLUR8Q9FqRH"},"source":["## Data pre-processing"]},{"cell_type":"markdown","metadata":{"id":"OPiT5ipeLMQ3"},"source":["### Vectorize the text\n","\n","Map the characters to vectors, so that the neural network can process it. In this case, we work at character level (i.e. generate sequences of characters).\n","\n","Store all unique characters in `vocabulary` and create two more data structures and one algorithm:\n","  * `char2idx`: a dictionary mapping from each unique character to its index in the list\n","  * `idx2char`: a numpy array of all unique characters in the text\n","  * `one_hot(.)`: a function to convert a character into its one-hot encoding"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33,"status":"ok","timestamp":1650470102493,"user":{"displayName":"Yang Li","userId":"01260798103734205900"},"user_tz":-60},"id":"84Sfh7EAkXMi","outputId":"6b10c087-6d30-4228-d15d-e9acec365836"},"outputs":[{"output_type":"stream","name":"stdout","text":["Vocabulary size: 68\n"]}],"source":["# compute the list of all unique characters in the file\n","text_set = set(text)\n","# the python file saving/reading discards the '\\r' character. add it back\n","text_set.add('\\r')\n","\n","vocabulary = sorted(text_set)\n","\n","VOCAB_SIZE = len(vocabulary)  # length of the vocabulary in chars\n","\n","# create the 2 data structures\n","char2idx = {u:i for i, u in enumerate(vocabulary)}\n","idx2char = np.array(vocabulary)\n","\n","# one-hot encoding\n","def one_hot(idx):\n","    oh = np.zeros(VOCAB_SIZE)\n","    oh[idx] = 1\n","    return oh\n","\n","print(\"Vocabulary size:\", VOCAB_SIZE)"]},{"cell_type":"markdown","source":["### Hyper parameters:"],"metadata":{"id":"MRvJOHVwdJQE"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"tzUAiMQxjPJy"},"outputs":[],"source":["# buffer size to shuffle our dataset, default 10000\n","BUFFER_SIZE = 10000  # @param {type: \"integer\"}\n","# batch size, default: 64\n","BATCH_SIZE =   64# @param {type: \"integer\"}\n","# number of RNN units, default 1024\n","N_RNN_UNITS = 1024  # @param {type: \"integer\"}\n","# length of the training data for the discriminator\n","MAX_LENGTH =   100# @param {type: \"integer\"}\n","# size of the embedding layer, default 256\n","EMBEDDING_DIM = 256    # @param {type: \"integer\"}"]},{"cell_type":"markdown","metadata":{"id":"zuRinUb-xc58"},"source":["### Create training data\n","Simply slice the data into chunks of length `MAX_LENGTH` and then create batches from the dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"54Yjx4f_kufc"},"outputs":[],"source":["vector_text = []\n","matrix_text = []\n","next_text = [] # for generator's pretraining\n","permuted_text = [] # for discriminator's pretraining\n","disc_label = [] # for discriminator's pretraining\n","\n","for c in range(0, len(text)-MAX_LENGTH, MAX_LENGTH):\n","    chunk = text[c : c + MAX_LENGTH]\n","    indices = [char2idx[i] for i in chunk]\n","    matrices = [one_hot(i) for i in indices]\n","    vector_text.append(indices)\n","    matrix_text.append(matrices)\n","\n","    # for generator's pretraining\n","    next_chunk = text[c + 1 : c + 1 + MAX_LENGTH]\n","    next_text.append([char2idx[i] for i in next_chunk])\n","\n","    # for discriminator's pretraining\n","    permuted = list(chunk)\n","    np.random.shuffle(permuted)\n","    permuted_text.append([one_hot(char2idx[i]) for i in permuted])\n","    permuted_text.append(matrices)\n","    disc_label += [0, 1]\n","\n","print(np.array(vector_text).shape)\n","print(np.array(matrix_text).shape)\n","print(np.array(permuted_text).shape)\n","print(np.array(disc_label).shape)\n","\n","# batch the dataset\n","dataset = tf.data.Dataset.from_tensor_slices((vector_text, matrix_text)).shuffle(BUFFER_SIZE)\n","dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n","\n","N_TRAIN_BATCHES = len(dataset)\n","\n","# for generator's pretraining\n","gen_dataset = tf.data.Dataset.from_tensor_slices((vector_text, next_text)).shuffle(BUFFER_SIZE)\n","gen_dataset = gen_dataset.batch(BATCH_SIZE, drop_remainder=True)\n","\n","# for discriminator's pretraining\n","disc_dataset = tf.data.Dataset.from_tensor_slices((permuted_text, disc_label)).shuffle(BUFFER_SIZE)\n","disc_dataset = disc_dataset.batch(BATCH_SIZE, drop_remainder=True)\n","\n","print(dataset)\n","print(gen_dataset)\n","print(disc_dataset)"]},{"cell_type":"markdown","metadata":{"id":"5jKrOdiwllce"},"source":["# Generative Adversarial Recurrent Neural Network (GARNN)"]},{"cell_type":"markdown","metadata":{"id":"to1H12deGZ9r"},"source":["## Generator Network\n","This network takes as input a chunk of text and produces a probability distribution over the vocabulary predicting the next character."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":435,"status":"ok","timestamp":1650470102908,"user":{"displayName":"Yang Li","userId":"01260798103734205900"},"user_tz":-60},"id":"Dhi7xMhIY_42","outputId":"5d23a287-543f-4317-abbd-b19cd2abf541"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_1 (Embedding)     (64, None, 256)           17408     \n","                                                                 \n"," gru_1 (GRU)                 (64, None, 1024)          3938304   \n","                                                                 \n"," dense_4 (Dense)             (64, None, 68)            69700     \n","                                                                 \n","=================================================================\n","Total params: 4,025,412\n","Trainable params: 4,025,412\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["# Define input and output around the RNN (GRU)\n","def Generator(batch_size=BATCH_SIZE, vocab_size=VOCAB_SIZE, embedding_dim=EMBEDDING_DIM, n_rnn_units=N_RNN_UNITS):\n","    model = tf.keras.Sequential(\n","        [\n","            tf.keras.layers.Embedding(\n","                vocab_size,\n","                embedding_dim,\n","                batch_input_shape=[batch_size, None]\n","            ),\n","            tf.keras.layers.GRU(\n","                n_rnn_units,\n","                return_sequences=True,\n","                stateful=True,\n","                recurrent_activation='sigmoid',\n","                recurrent_initializer='glorot_uniform'\n","            ),\n","            tf.keras.layers.Dense(vocab_size, activation=\"softmax\")\n","        ]\n","    )\n","    return model\n","\n","# Define the loss function\n","def gen_loss_function(labels, pred):\n","    return tf.keras.losses.sparse_categorical_crossentropy(labels, pred)\n","\n","gen = Generator()\n","\n","# Define the optimiser\n","# default: 0.001\n","learning_rate = 0.001  #@param{type:\"raw\"}\n","# default: 0.5\n","beta = 0.5 #@param{type:\"raw\"}\n","gen_optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=beta)\n","\n","# Build the network\n","gen.build((BATCH_SIZE, None, VOCAB_SIZE))  # specifies input shape\n","\n","gen.summary()"]},{"cell_type":"markdown","metadata":{"id":"V8g6ntzMlBhP"},"source":["## Discriminator Network\n","This network takes as input a chunk of text and guess whether it is generated by the generator network."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":460,"status":"ok","timestamp":1650472208749,"user":{"displayName":"Yang Li","userId":"01260798103734205900"},"user_tz":-60},"id":"oyoteKQ4eqwU","outputId":"d2ef9065-a448-44c8-a6cc-c719e37f48be"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_7\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_9 (InputLayer)           [(64, 100, 68)]      0           []                               \n","                                                                                                  \n"," dense_24 (Dense)               (64, 100, 256)       17408       ['input_9[0][0]']                \n","                                                                                                  \n"," tf.expand_dims_8 (TFOpLambda)  (64, 100, 256, 1)    0           ['dense_24[0][0]']               \n","                                                                                                  \n"," conv2d_44 (Conv2D)             (64, 98, 1, 32)      24608       ['tf.expand_dims_8[0][0]']       \n","                                                                                                  \n"," conv2d_45 (Conv2D)             (64, 94, 1, 32)      57376       ['tf.expand_dims_8[0][0]']       \n","                                                                                                  \n"," conv2d_46 (Conv2D)             (64, 90, 1, 32)      90144       ['tf.expand_dims_8[0][0]']       \n","                                                                                                  \n"," conv2d_47 (Conv2D)             (64, 86, 1, 32)      122912      ['tf.expand_dims_8[0][0]']       \n","                                                                                                  \n"," conv2d_48 (Conv2D)             (64, 82, 1, 32)      155680      ['tf.expand_dims_8[0][0]']       \n","                                                                                                  \n"," conv2d_49 (Conv2D)             (64, 78, 1, 32)      188448      ['tf.expand_dims_8[0][0]']       \n","                                                                                                  \n"," max_pooling2d_44 (MaxPooling2D  (64, 1, 1, 32)      0           ['conv2d_44[0][0]']              \n"," )                                                                                                \n","                                                                                                  \n"," max_pooling2d_45 (MaxPooling2D  (64, 1, 1, 32)      0           ['conv2d_45[0][0]']              \n"," )                                                                                                \n","                                                                                                  \n"," max_pooling2d_46 (MaxPooling2D  (64, 1, 1, 32)      0           ['conv2d_46[0][0]']              \n"," )                                                                                                \n","                                                                                                  \n"," max_pooling2d_47 (MaxPooling2D  (64, 1, 1, 32)      0           ['conv2d_47[0][0]']              \n"," )                                                                                                \n","                                                                                                  \n"," max_pooling2d_48 (MaxPooling2D  (64, 1, 1, 32)      0           ['conv2d_48[0][0]']              \n"," )                                                                                                \n","                                                                                                  \n"," max_pooling2d_49 (MaxPooling2D  (64, 1, 1, 32)      0           ['conv2d_49[0][0]']              \n"," )                                                                                                \n","                                                                                                  \n"," concatenate_8 (Concatenate)    (64, 1, 1, 192)      0           ['max_pooling2d_44[0][0]',       \n","                                                                  'max_pooling2d_45[0][0]',       \n","                                                                  'max_pooling2d_46[0][0]',       \n","                                                                  'max_pooling2d_47[0][0]',       \n","                                                                  'max_pooling2d_48[0][0]',       \n","                                                                  'max_pooling2d_49[0][0]']       \n","                                                                                                  \n"," tf.compat.v1.squeeze_11 (TFOpL  (64, 192)           0           ['concatenate_8[0][0]']          \n"," ambda)                                                                                           \n","                                                                                                  \n"," dense_25 (Dense)               (64, 128)            24704       ['tf.compat.v1.squeeze_11[0][0]']\n","                                                                                                  \n"," dense_26 (Dense)               (64, 1)              129         ['dense_25[0][0]']               \n","                                                                                                  \n"," tf.compat.v1.squeeze_12 (TFOpL  (64,)               0           ['dense_26[0][0]']               \n"," ambda)                                                                                           \n","                                                                                                  \n","==================================================================================================\n","Total params: 681,409\n","Trainable params: 681,409\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"]}],"source":["# Define input and output\n","def Discriminator(vocab_size=VOCAB_SIZE, batch_size=BATCH_SIZE, embedding_dim=EMBEDDING_DIM):\n","    \n","    inp = tf.keras.Input((MAX_LENGTH, vocab_size), batch_size=batch_size)\n","    x = tf.keras.layers.Dense(embedding_dim, use_bias=False)(inp)\n","    sentence = tf.expand_dims(x, -1)\n","\n","    x1 = tf.keras.layers.Conv2D(32, (3, embedding_dim), activation=\"relu\")(sentence)\n","    x1 = tf.keras.layers.MaxPooling2D(pool_size=(MAX_LENGTH-2, 1))(x1)\n","    x2 = tf.keras.layers.Conv2D(32, (7, embedding_dim), activation=\"relu\")(sentence)\n","    x2 = tf.keras.layers.MaxPooling2D(pool_size=(MAX_LENGTH-6, 1))(x2)\n","    x3 = tf.keras.layers.Conv2D(32, (11, embedding_dim), activation=\"relu\")(sentence)\n","    x3 = tf.keras.layers.MaxPooling2D(pool_size=(MAX_LENGTH-10, 1))(x3)\n","    x4 = tf.keras.layers.Conv2D(32, (15, embedding_dim), activation=\"relu\")(sentence)\n","    x4 = tf.keras.layers.MaxPooling2D(pool_size=(MAX_LENGTH-14, 1))(x4)\n","    x5 = tf.keras.layers.Conv2D(32, (19, embedding_dim), activation=\"relu\")(sentence)\n","    x5 = tf.keras.layers.MaxPooling2D(pool_size=(MAX_LENGTH-18, 1))(x5)\n","    x6 = tf.keras.layers.Conv2D(32, (23, embedding_dim), activation=\"relu\")(sentence)\n","    x6 = tf.keras.layers.MaxPooling2D(pool_size=(MAX_LENGTH-22, 1))(x6)\n","\n","    conc = tf.keras.layers.Concatenate()([x1, x2, x3, x4, x5, x6])\n","    \n","    x = tf.squeeze(conc)\n","    \n","    x = tf.keras.layers.Dense(128)(x)\n","    y = tf.keras.layers.Dense(1)(x)\n","    y = tf.squeeze(y)\n","\n","    model = tf.keras.Model(inputs=inp, outputs=y)\n","    \n","    return model\n","\n","# Define the loss function\n","def disc_loss_function(labels, logits):\n","    return tf.keras.losses.binary_crossentropy(labels, logits, from_logits=True)\n","\n","disc = Discriminator()\n","\n","# Define the optimiser\n","# default: 0.001\n","learning_rate = 0.001  #@param{type:\"raw\"}\n","# default: 0.5\n","beta = 0.5 #@param{type:\"raw\"}\n","disc_optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=beta)\n","\n","# Build the network\n","disc.build((BATCH_SIZE, None, VOCAB_SIZE))  # specifies input shape\n","\n","disc.summary()"]},{"cell_type":"markdown","metadata":{"id":"guqL3J3Blcjv"},"source":["## Putting them together: Generative Adversarial Network"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iPob3vIlLcqa"},"outputs":[],"source":["# A GAN class as an extension of the tf.keras.Model object\n","\n","# Define the loss function (BCE) \n","def gan_loss(logits, is_real=True):\n","    \"\"\"\n","    Computes cross entropy between logits and labels\n","    \"\"\"\n","    if is_real:\n","        labels = tf.ones_like(logits)\n","    else:\n","        labels = tf.zeros_like(logits)\n","\n","    # Returns loss calculation\n","    return tf.nn.sigmoid_cross_entropy_with_logits(labels, logits)\n","\n","class GARNN(tf.keras.Model):\n","    \"\"\" \n","    A basic GAN class. Extends tf.keras.Model\n","    \"\"\"\n","\n","    def __init__(self, **kwargs):\n","        super(GARNN, self).__init__()\n","        self.__dict__.update(kwargs)\n","\n","        self.gen = self.gen\n","        self.disc = self.disc\n","\n","    def call(self, x):\n","        return self.gen(x)\n","\n","    def generate(self, z):\n","        \"\"\"\n","        Run input vector z through the generator to create fake data.\n","        \"\"\"\n","        return self.gen(z)\n","\n","    def discriminate(self, x):\n","        \"\"\"\n","        Run data through the discriminator to label it as real or fake.\n","        \"\"\"\n","        return self.disc(x)\n","\n","    def compute_loss(self, x):\n","        \"\"\" \n","        Passes through the network and computes loss for given data.\n","        \"\"\"\n","        vector = x[0]\n","        matrix = x[1]\n","        # Use the data to generate a fake data set with the generator network.\n","        self.gen.reset_states()\n","        fakes = self.generate(vector)\n","\n","        # Use the discriminator network to obtain labels for both the generated data (x_gen) and the real data (x)\n","        logits_reals = self.discriminate(matrix)\n","        logits_fakes = self.discriminate(fakes)\n","\n","        # Discriminator loss, looking at correctly labeled data\n","        # Losses of the real data with correct label \"1\"\n","        disc_real_loss = gan_loss(logits=logits_reals, is_real=True)\n","        # Losses of the fake data with correct label \"0\"\n","        disc_fake_loss = gan_loss(logits=logits_fakes, is_real=False)\n","        # The discriminator loss is the sum of the 2 previous values\n","        disc_loss = disc_fake_loss + disc_real_loss\n","\n","        # Generator loss, looking at the fake data labeled as real (\"1\")\n","        gen_loss = gan_loss(logits=logits_fakes, is_real=True)\n","\n","        # Return losses\n","        return disc_loss, gen_loss\n","\n","    def compute_gradients(self, x):\n","        \"\"\" \n","        Passes through the network and computes gradients.\n","        \"\"\"\n","        ### Pass x through network and compute losses\n","        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n","            disc_loss, gen_loss = self.compute_loss(x)\n","\n","        # Compute gradients\n","        gen_gradients = gen_tape.gradient(gen_loss, self.gen.trainable_variables)\n","        disc_gradients = disc_tape.gradient(disc_loss, self.disc.trainable_variables)\n","\n","        return gen_gradients, disc_gradients\n","\n","    def apply_gradients(self, gen_gradients, disc_gradients):\n","        \"\"\"\n","        Apply given gradients to both networks.\n","        \"\"\"\n","        self.gen_optimizer.apply_gradients(zip(gen_gradients, self.gen.trainable_variables))\n","        self.disc_optimizer.apply_gradients(zip(disc_gradients, self.disc.trainable_variables))\n","\n","    @tf.function\n","    def train(self, train_x):\n","        \"\"\"\n","        Train the GAN!\n","        \"\"\"\n","        gen_gradients, disc_gradients = self.compute_gradients(train_x)\n","        self.apply_gradients(gen_gradients, disc_gradients)"]},{"cell_type":"markdown","metadata":{"id":"BZ02CKORu7k4"},"source":["# Training\n","While the GAN model set up takes care of a training iteration, we need to repeat this for several epochs and observe how the quality of text generated improves over time. The training is done in batches at each epoch, using the `tqdm` library to create these according to variables defined earlier and display progress."]},{"cell_type":"markdown","source":["## Pretraining\n","Train the generator and the discriminator before the adversarial game."],"metadata":{"id":"USbWj-dZybXu"}},{"cell_type":"markdown","source":["### Generator\n","Train it as an auto-regressive RNN for prediction next character in the sequence."],"metadata":{"id":"shCNZ5ebyi0W"}},{"cell_type":"code","source":["gen.compile(gen_optimizer, gen_loss_function)\n","\n","# default: 50\n","n_epochs =  50 # @param{type: \"integer\"} \n","# history = model.fit(dataset, epochs=n_epochs, callbacks=[checkpoint_callback])\n","history = gen.fit(gen_dataset, epochs=n_epochs)"],"metadata":{"id":"FC_OI1tgyeVl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Discriminator\n","Train it to classified shuffled sentences and the normal ones."],"metadata":{"id":"Kf-ezn8bymWT"}},{"cell_type":"code","source":["# Compile the model\n","disc.compile(disc_optimizer, loss=disc_loss_function, metrics=['binary_accuracy'])\n","\n","# default: 50\n","n_epochs =  5 # @param{type: \"integer\"} \n","# history = model.fit(dataset, epochs=n_epochs, callbacks=[checkpoint_callback])\n","history = disc.fit(disc_dataset, epochs=n_epochs)"],"metadata":{"id":"UBJ4Rgy3ymr_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Adversarial training"],"metadata":{"id":"GZufrkDhyer1"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZjucpxPxvEQD"},"outputs":[],"source":["# Set up the model\n","model = GARNN(\n","    gen = gen,\n","    disc = disc,\n","    gen_optimizer = gen_optimizer,\n","    disc_optimizer = disc_optimizer\n",")\n","\n","# default: 50\n","n_epochs =  5 #@param{type: \"integer\"} \n","\n","# losses = pd.DataFrame(columns = ['disc_loss', 'gen_loss'])\n","\n","for epoch in range(1, n_epochs + 1):\n","\n","    print(\"Epoch: {}\".format(epoch))\n","\n","    # Train the model\n","    for batch, train_x in tqdm(zip(range(N_TRAIN_BATCHES), dataset), total=N_TRAIN_BATCHES):\n","        model.train(train_x)"]},{"cell_type":"markdown","source":["Saving the model. Make sure the path exists"],"metadata":{"id":"h-3EoH3WFIa6"}},{"cell_type":"code","source":["drive.mount('/content/gdrive')\n","path = \"/content/gdrive/My Drive/Colab Notebooks/computational creativity/GARNN/\""],"metadata":{"id":"PZhnIhjPbCgw"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RdEt_dhTQugw"},"outputs":[],"source":["# Save trained models\n","gen.save(path + \"models/gen.h5\")\n","disc.save(path + \"models/disc.h5\")"]},{"cell_type":"markdown","source":["# Testing"],"metadata":{"id":"UO7cOvJDFD80"}},{"cell_type":"markdown","source":["## Set up text generation function:"],"metadata":{"id":"v3gTrEyfFPkJ"}},{"cell_type":"code","source":["def generate_text(model, input_text, n_characters_output=1000):\n","    # First, vectorize the input text as before\n","    input_eval = [char2idx[s] for s in input_text]\n","    input_eval = tf.expand_dims(input_eval, 0)\n","\n","    # We'll store results in this variable\n","    text_generated = []\n","\n","    # Generate the number of characters desired\n","    model.reset_states()\n","    for i in range(n_characters_output):\n","        # Run input through model\n","        predictions = model(input_eval)\n","\n","        # Remove the batch dimension\n","        predictions = predictions[0][-1]\n","\n","        # Using argmax to predict the character returned by the model\n","        predicted_id = np.argmax(predictions)\n","\n","        # Pass the predicted character as the next input to the model\n","        input_eval = tf.expand_dims([predicted_id], 0)\n","\n","        # Add the predicted character to the output\n","        text_generated.append(idx2char[predicted_id])\n","\n","    # Return output\n","    return (input_text + ''.join(text_generated))"],"metadata":{"id":"T2sjc_CaFSK6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Generate text"],"metadata":{"id":"OsIwGFT4FT02"}},{"cell_type":"markdown","source":["Load the model."],"metadata":{"id":"2FJPLh2xFLap"}},{"cell_type":"code","source":["'''\n","import gdown, os\n","\n","url = \"https://drive.google.com/drive/folders/1R3qslZrX4lE0jRi3lf3Bik_rlLdDjMht\"\n","\n","download_successful = None # A workaround to make sure that gdown downloads the whole folder successfully, see https://github.com/wkentaro/gdown/issues/43\n","while download_successful == None:\n","  download_successful = gdown.download_folder(url, quiet=True, use_cookies=False)\n","  os.system('rm ~/.cache/gdown/cookies.json')\n","\n","gen = Generator(batch_size=1)\n","gen.load_weights(\"/content/GARNN/models/gen.h5\")\n","# disc.load_weights(\"/content/GARNN/models/disc.h5\")\n","'''"],"metadata":{"id":"XlsCrkyiixjq"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W2ydzoIv5P4s"},"outputs":[],"source":["# Re-load trained networks\n","\n","gen.load_weights(path + \"models/gen.h5\")\n","disc.load_weights(path + \"models/disc.h5\")"]},{"cell_type":"markdown","source":["Give some input text, the AI will continue writing on it..."],"metadata":{"id":"4eCA8gKEY3w_"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"uQCWAq3LFrr8","executionInfo":{"status":"ok","timestamp":1650473618717,"user_tz":-60,"elapsed":2508,"user":{"displayName":"Yang Li","userId":"01260798103734205900"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"88005b9d-a07b-4930-e501-e8f007fbf931"},"outputs":[{"output_type":"stream","name":"stdout","text":["Generated text:\n","to be or not to be, that is the question. it was\r\n","      more than civil; it was really attentive; and then i must\r\n","      speak plainly. if you, my dear father, i congratulate you.”\r\n","\r\n","      “and this is also different from your affection, you make\r\n","      elizabeth treated with the rest of the house, as soon as mr. bennet were\r\n","      to make a small inconvenience to herself and her daughter, to whom i have\r\n","      related the affair of making \n"]}],"source":["input_text = \"to be or not to be, that is the question.\" # @param {type:\"string\"}\n","input_text = input_text.lower()\n","\n","n_output_characters = 404 #@param {type:\"integer\"}\n","\n","output_text = generate_text(gen, input_text, n_output_characters)\n","\n","print(\"Generated text:\", output_text, sep='\\n')"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"“GARNN.ipynb”的副本","provenance":[{"file_id":"1eBbC_UZzLzBw-S_K-w5VzHZMkgVGS18T","timestamp":1650473924672},{"file_id":"1o7J2n7gJ6LDckDZ0Ec5xm7JWMZLHBZGP","timestamp":1650363729881},{"file_id":"1dzKtN9ZqEz9998sClwPpA5xa7xy4H_FD","timestamp":1649944883581},{"file_id":"1zd5MsRxoQj1vcEiTORVavDkH4qjuOuAf","timestamp":1649886188199}],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}